---
title: "Milestone Report"
author: "Mike Hulin"
date: "July 11, 2018"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This project consists of developing a predictive text model to determine the next word the user will type.
The model will be constructed by using text samples from 3 sources: Twitter, Blogs, and News reports. This milestone 
report will show the general make up of the sample files and the approach for building the model.

```{r get_data, include=TRUE, warning=FALSE, message=FALSE, results="hide", echo=FALSE}
library(quanteda)
library(readtext)
library(tm)
library(ggplot2)
library(R.utils)
library(dplyr)
library(parallel)
library(stringi)
library(stringr)
library(tidytext)
library(tidyr)
library(qdap)
library(wordcloud2)

#options(java.parameters = "-Xmx2048m")
quanteda_options(threads = 7)

setwd("~/Coursera/Capstone/Project")
doc.twitter <- "~/Coursera/Capstone/Project/data/en_US/en_US.twitter.txt"
doc.blog <- "~/Coursera/Capstone/Project/data/en_US/en_US.blogs.txt"
doc.new <- "~/Coursera/Capstone/Project/data/en_US/en_US.news.txt"
doc.badwords <- "~/Coursera/Capstone/Project/data/badwords.txt"

#df.1gram <- readRDS(file <- 'df.1gram.rds')
#df.2gram <- readRDS(file <- 'df.2gram.rds')
#df.3gram <- readRDS(file <- 'df.3gram.rds')
twitter.con <- file(doc.twitter, open = "rb")
twitter <- readLines(twitter.con, skipNul = TRUE, encoding="UTF-8")
close(twitter.con)

news.con <- file(doc.new, open = "rb")
news <- readLines(news.con, skipNul = TRUE, encoding="UTF-8")
close(news.con)

blog.con <- file(doc.blog, open = "rb")
blogs <- readLines(blog.con, skipNul = TRUE, encoding="UTF-8")
close(blog.con)

# Load bad words file for profanity filtering
bw.con <- file(doc.badwords, open = "rb") 
badwords <- readLines(bw.con, skipNul = TRUE, encoding="UTF-8")
close(bw.con); rm(bw.con)

## Clean up input files
rm(doc.twitter, doc.blog, doc.new, doc.badwords,  twitter.con, blog.con, news.con)

    ### Load Data Cleaning Functions  ####
removeURL <- function(x) gsub("http|www[[:alnum:][:punct:]]*", "", x) 

clean_text <- function (textcp) {
  textcp <- str_replace_all(textcp, "[^0-9A-Za-z///' ]", "")  ## needed to remove hex codes
  textcp <- removeURL(textcp)
  textcp <- bracketX(textcp)
  textcp <- stripWhitespace(textcp)
  textcp <- replace_contraction(textcp)
  textcp <- removeNumbers(textcp)
  textcp <- replace_symbol(textcp)
  textcp <- tolower(textcp)
  textcp <- removeWords(textcp, badwords)
  textcp
}

clean_corpus <- function (textcp) {
  textcp <- tm_map(textcp, stripWhitespace)
  textcp <- tm_map(textcp, removeNumbers)
  textcp
}


 ### Function to make the corpus that includes the cleaning steps on the tesxt file  ###
make_corpus <- function (sample) {
  sample_clean <- clean_text(sample)
  s_corpus <- corpus(sample_clean)
  return(s_corpus)
}


# Tokenize ####
fun.tokenize = function(x, ngramSize = 1) {
  
  # Remove unwanted items with Quanteda tokens options
  tolower(
    tokens(x,
           what = "word",
           remove_url = TRUE,
           remove_numbers = TRUE,
           remove_punct = TRUE,
           remove_twitter = TRUE,
           ngrams = ngramSize,
           concatenator = " "))
}

# Frequency tables ####
fun.frequency = function(x, minCount = 1) {
  x = x %>%
    group_by(NextWord) %>%
    summarize(count = n()) %>%
    filter(count >= minCount)
  x = x %>% 
    mutate(freq = count / sum(x$count)) %>% 
    select(-count) %>%
    arrange(desc(freq))
}

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


### Data Analysis Steps

1. Read in the data sources
2. Examine the data sources for size 
3. Reduce the size  and collate the data into one sample
  - involves data cleaning
4. Tokenize the data
5. Compute the term frequency for the sample 

Initially, I took the approach of using the tm and rweka packages for analysis by due to the size of the text samples, 
my computer could not process them without running into memory issues.   I have a very beefy computer, I7 processor with 64GB
of RAM.  

The Quanteda package was a great alternative and offered some unique features to help with preparing the data for the models.


### Data size

The following table is a good summary of the text files.   You can see that the total number of words for these data sets are about 100 million.   This count will be reduced through sampling and tokenization of the data.

```{r, echo=FALSE}
blog.size <- format(object.size(blogs), "Mb", digits = 3)
news.size <- format(object.size(news), "Mb", digits = 3)
twit.size <- format(object.size(twitter), "Mb", digits = 3)

blog.words <- stri_count_words(blogs)
news.words <- stri_count_words(news)
twit.words <- stri_count_words(twitter)

data.frame(source = c("blogs", "news", "twitter"),
           file.size.MB = c(blog.size, news.size, twit.size),
           num.lines = c(length(blogs), length(news), length(twitter)),
           num.words = c(sum(blog.words), sum(news.words), sum(twit.words)),
           max.line = c(max(nchar(blogs)), max(nchar(news)), max(nchar(twitter))),
           mean.num.words = c(mean(blog.words), mean(news.words), mean(twit.words)))
```


### Sampling

After some testing I decided to use a 5% sample, the three samples are combined into one inclusive sample.


```{r}
####################  SAMPLING the Files ###############################

    ## Sampling Text files Function ###
sampletext <- function(textbody, portion) {
  keep <- sample(1:length(textbody), length(textbody)*portion)
  Sampletext <- textbody[keep]
  Sampletext
}

# sampling text files 
set.seed(62618)
portion <- 0.05
SampleTwitter <- sampletext(twitter, portion)
SampleBlog <- sampletext(blogs, portion)
SampleNews <- sampletext(news, portion)

# combine sampled texts into one variable
SampleAll <- c(SampleBlog, SampleNews, SampleTwitter)

# Clean up files
rm(SampleBlog, SampleNews, SampleTwitter, blogs, twitter, news)

# write sampled texts into text files for further analysis
# writeLines(SampleAll, "data/SampleAll.txt")

# sample.con <- file("~/Coursera/Capstone/Project/data/SampleAll.txt", open = "rb")
# SampleAll <- readLines(sample.con, skipNul = TRUE, encoding="UTF-8")
# close(sample.con); rm(sample.con)

# clean the sample text
s_clean <- clean_text(SampleAll)

# Save the file for quicker processing
# writeLines(s_clean, "~/Coursera/Capstone/Project/data/Sampleclean.txt")
```
Before additional cleaning, this sample contains `r sum(stri_count_words(SampleAll))` words.  Cleaning
removes  `r sum(stri_count_words(SampleAll)) - sum(stri_count_words(s_clean))` words from the sample.  

For additional data analysis of the sample, I create a Document Frequency Matrix with the Quanteda package. 

```{r make_dfm}
clean_dfm <- dfm(s_clean)
format(object.size(clean_dfm), "Mb", digits = 2)
```

The Quanteda package can use the DFM file to provide some text statistics.   It can show the relationship between the top 50 words in a Frequency Co-location Matrix.   This is not that interesting of a graph, since it shows the most common words used in the english language.   For the purpose of this task, I did not filter out these common "stop words" since the goal is word prediction.   Additionally, a wordcloud of the top words in the sample was also created.

```{r}
# Cool Map
new_fcm <- fcm(clean_dfm)
feat <- names(topfeatures(new_fcm, 50))
new_fcm <- fcm_select(new_fcm, feat)
dim(new_fcm)
 
size <- log(colSums(dfm_select(clean_dfm, feat)))
textplot_network(new_fcm, min_freq = 0.8, vertex_size = size / max(size) * 3)

textplot_wordcloud(clean_dfm, max_words = 100)

```
### N-Gram Analysis

Analyzing the word frequency in the sample shows the expected top words.  These would be removed if standard text preparation was performed by removing common stopwords.   Since the goal is to make a next word prediction, I can't remove these from the sample.

```{r}
## ngrams using Quanteda package 
toks <- tokens(s_clean, remove_numbers = TRUE, remove_punct = TRUE,
               remove_symbols = TRUE, remove_separators = TRUE,
               remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE,
               ngrams = 1, concatenator = " ")

ngram_dfm <- dfm(toks)
ngram_dfm <- dfm_sort(ngram_dfm, decreasing = T)
head(docfreq(ngram_dfm))
barplot(head(topfeatures(ngram_dfm), 20))
```


I created 1, 2, 3, and 4 grams from the sample and plotted the most frequent terms.

```{r}
## ngrams using Quanteda package 
plot_ngram <- function (x, ngram_size=1) {
  ngram_plot <- x %>%
  tokens(remove_numbers = TRUE, remove_punct = TRUE,
               remove_symbols = TRUE, remove_separators = TRUE,
               remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE,
               ngrams = ngram_size, concatenator = " ") %>%
  dfm() %>%
  dfm_sort(decreasing = T) %>%
  topfeatures( 20) 
  ## convert results to a data frame
  dat <- data.frame(term = names(ngram_plot), value =ngram_plot, row.names=NULL) 
  
  ggplot(data = dat,  aes(x = reorder(term,value), y = value)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(x = NULL, y = "Relative frequency")

}

p1 <- plot_ngram(s_clean, 1)
p2 <- plot_ngram(s_clean, 2)
p3 <- plot_ngram(s_clean, 3)
p4 <- plot_ngram(s_clean, 4)

multiplot(p1, p2, p3, p4,  cols = 2)
```

Eventually the prediction model will take into account the frequency that each word occurs in the sample.  It would be good to know how many words only occur once in the sample.   Analysis was performed to determine these stats for 1-gram and 3-gram sets of data.
```{r}
## find the single occurances of words

find_unigram <- function (x, ngram_size=1) {
  ngram_plot <- x %>%
  tokens(remove_numbers = TRUE, remove_punct = TRUE,
               remove_symbols = TRUE, remove_separators = TRUE,
               remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE,
               ngrams = ngram_size, concatenator = " ") %>%
  dfm() %>%
  dfm_sort(decreasing = F)
}

freq <- textstat_frequency(find_unigram(s_clean, 1))  

ggplot(freq, aes(x=frequency, y=rank)) + geom_point()
```
This plot shows the distribution of the sorted frequency matrix of unigrams in the sample.  It show that a majority of the unigrams occur very infrequently.

In the sample there are `r nrow(freq[freq$frequency ==1, ])`  samples that occur only once.  This accounts for
`r (nrow(freq[freq$frequency ==1, ])/nrow(freq))*100 ` percent of the total.  

```{r}

x <- cut(freq$frequency, 3, include.lowest = TRUE, labels=c("Low", "Med", "High"))
table(x)
```
Using the 'cut' function and dividing the frequencies into three buckets, gives another view of the skewed distribution.

 

```{r}
freq <- textstat_frequency(find_unigram(s_clean, 3))  

ggplot(freq, aes(x=frequency, y=rank)) + geom_point()
```
```{r}

x <- cut(freq$frequency, 3, include.lowest = TRUE, labels=c("Low", "Med", "High"))
table(x)
```
Plotting the tri-grams results in basically the same distribution.  In the sample there are `r nrow(freq[freq$frequency ==1, ])`  samples that occur only once.  This accounts for
`r (nrow(freq[freq$frequency ==1, ])/nrow(freq))*100 ` percent of the total.   



### Word Coverage

The github repository is useful as a corpus for typing training programs. According to analysis of the Oxford English Corpus, the 7,000 most common English lemmas account for approximately 90% of usage, so a 10,000 word training corpus is more than sufficient for practical training applications.

To check the coverage of my sample, I am using a 10,000 word corpus found here:  https://github.com/first20hours/google-10000-english/blob/master/google-10000-english-usa.txt

To check the coverage of the 5% sample.  I used the dfm_keep package and kept only the words in the Google 10,000 word list.  Additionaly processing with textstat_frequency, resulted in a term frequency for the Top 10K google words.

```{r}
len.s_clean <- length(s_clean)
my_dict <- readr::read_table("https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa.txt", col_names = F)
popword <- as.list(my_dict$X1)

goodwords <- quanteda::dfm_keep(clean_dfm, popword)  
gw.freq <- textstat_frequency(goodwords)
summary(gw.freq$frequency)
```

The current sample covers `r length(gw.freq$feature)` words.   I am confident in this sample containing enough words to build the model with in the next stages.

### Future Steps

Model selection will most likely be the generation of n-grams and using a back off model to predict the next word.






